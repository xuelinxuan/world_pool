
name: CD

permissions:
  id-token: write
  contents: read

# on:
#   workflow_run:
#     workflows: ["CI"]   # 必须和 CI workflow 的 name 完全一致
#     types: [completed]
#     branches: [main]       # 只有 main 上的 CI 完成才触发

on:
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  deploy:
    name: II.CD run_code
    # if: ${{ github.event.workflow_run.conclusion == 'success' }}  # 仅当 CI 成功
    runs-on: ubuntu-latest
    steps:
    - name: 0.Check out Git repository
      uses: actions/checkout@v4

    - name: 1.Configure aws credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ secrets.AWS_REGION }}



    - name: 2.Start EC2 instance
      run: |
        aws ec2 start-instances         --instance-ids ${{ secrets.EC2_INSTANCE_ID }}
        #确认连接成功
        aws ec2 wait instance-status-ok --instance-ids ${{ secrets.EC2_INSTANCE_ID }}
        echo "✅ EC2 instance is running"

    - name: 3.Deploy DAGs to EC2 (SCP)
      uses: appleboy/scp-action@v0.1.7
      with:
        host    : ${{ secrets.SSH_HOST }}
        username: ${{ secrets.SSH_USER }}
        key     : ${{ secrets.SSH_KEY }}        # 部署专用私钥
        source  : dags/
        target  : /home/ubuntu/world_pool/dags/


    - name: 3.Setup Airflow via SSM
      env:
        DAGS_BRANCH: main 
      run: |
        set -euo pipefail

        DAGS_BRANCH="${DAGS_BRANCH:-main}"
    
        jq -n --arg BR "${DAGS_BRANCH}" '{
          commands: [
            "set -eu",
            "export HOME=/root",
            
            # 避免 root 下 git dubious ownership
            "git config --global --add safe.directory /home/ubuntu/world_pool || true",
            "git config --global --add safe.directory /home/ubuntu/world_pool/dags || true",
            "cd /home/ubuntu/world_pool/dags",
            # 拉最新代码（用 --ff-only 防止产生合并提交；你也可改成 fetch/reset）
            "git fetch --all --prune",
            "git checkout --quiet " + $BR + " || true",
            "git pull --ff-only origin " + $BR,
    
            # 回到 compose 目录并启动/刷新容器
            "cd /home/ubuntu/world_pool",
            "sudo systemctl start docker || true",
            "sudo docker compose up -d --remove-orphans",
            "sudo docker compose ps"
            ]
          }' > params.json

        # jq -n --arg s "$SCRIPT" '{commands: [$s]}' > params.json

        CMD_ID=$(aws ssm send-command \
          --targets "Key=instanceIds,Values=${{ secrets.EC2_INSTANCE_ID }}" \
          --document-name "AWS-RunShellScript" \
          --comment "Start/Update Airflow (compose v2)" \
          --region "${{ secrets.AWS_REGION }}" \
          --parameters file://params.json \
          --query "Command.CommandId" --output text)
          
        echo "SSM Command ID: $CMD_ID"

        # 1) 等待远端执行结束（即使失败也先别中断）
        set +e
        aws ssm wait command-executed \
          --command-id "$CMD_ID" \
          --instance-id "${{ secrets.EC2_INSTANCE_ID }}" \
          --region "${{ secrets.AWS_REGION }}"
        STATUS=$?     # 0=Success，非0=Failed/TimedOut/Canceled
        set -e        # 2) 立刻恢复“遇错即停”
        
        # 3) 无论成功失败，都把远端日志取回来便于排查
        aws ssm get-command-invocation \
          --command-id "$CMD_ID" \
          --instance-id "${{ secrets.EC2_INSTANCE_ID }}" \
          --region "${{ secrets.AWS_REGION }}" \
          --query '{Status:Status, StdOut:StandardOutputContent, StdErr:StandardErrorContent}' \
          --output json
        
        # 4) 用远端执行结果作为当前 step 的退出码
        exit $STATUS
          
    - name: 4.Run Airflow dag
      env:
        AIRFLOW_HOST: ${{ secrets.AIRFLOW_HOST }}
        AIRFLOW_USER: ${{ secrets.AIRFLOW_USER }}
        AIRFLOW_PASS: ${{ secrets.AIRFLOW_PASS }}
        DAG_ID      : market_history
      run: |
        set -euo pipefail
    
        # 1) 生成本次运行的 dag_run_id（自己可控，无需 URL 编码）
        DAG_RUN_ID="manual__$(date -u +%s)"
        echo "Triggering DAG: ${DAG_ID} with DAG_RUN_ID=${DAG_RUN_ID}"
    
        # 2) 触发 DAG
        curl -fSs -X POST "http://${AIRFLOW_HOST}:8080/api/v1/dags/${DAG_ID}/dagRuns" \
          -H "Content-Type: application/json" \
          -u "${AIRFLOW_USER}:${AIRFLOW_PASS}" \
          -d "{\"dag_run_id\": \"${DAG_RUN_ID}\", \"conf\": {\"key\": \"value\"}}"
    
        # 3) 轮询状态（每 50 秒查一次，最多 30 次 ≈ 25 分钟）
        for i in {1..30}; do
          STATE=$(curl -fsS -u "${AIRFLOW_USER}:${AIRFLOW_PASS}" \
            "http://${AIRFLOW_HOST}:8080/api/v1/dags/${DAG_ID}/dagRuns/${DAG_RUN_ID}" | jq -r '.state')
    
          echo "[$i] DAG run state: ${STATE:-<no state>}"
          if [[ "$STATE" == "success" ]]; then
            echo "✅ DAG succeeded"
            exit 0
          elif [[ "$STATE" == "failed" ]]; then
            echo "❌ DAG failed"
            exit 1
          fi
          sleep 10
        done
    
        echo "⚠️ DAG timed out"
        exit 1

    # - name: 5.Stop EC2 instance
    #   if: always()
    #   run: |
    #     echo "🛑 Stopping EC2 ${{ secrets.EC2_INSTANCE_ID }} ..."
    #     aws ec2 stop-instances --instance-ids ${{ secrets.EC2_INSTANCE_ID }}
    #     aws ec2 wait instance-stopped --instance-ids ${{ secrets.EC2_INSTANCE_ID }}
    #     echo "✅ EC2 instance stopped"



          
  
